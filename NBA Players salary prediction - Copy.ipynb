{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    " <FONT COLOR=\"BLUE\">\n",
    " <strong><font size = '+5'>  Salary Prediction of NBA basketball players </font></strong>\n",
    "</FONT>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src= \"images/NBA-vector-logos.jpeg\"\n",
    "    width = 40%;\n",
    "    height = auto; />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Team: 6 Layers of Stonks**\n",
    "- **Nada Amini** (M2 Data Science_École Polytechnique)\n",
    "- **Nhat-Minh Dao** (M2 Data Science_École Polytechnique)\n",
    "- **Fares Feki** (M2 Data Science_École Polytechnique)\n",
    "- **Juhyun Kim** (M2 Data Science_École Polytechnique)\n",
    "- **Zongmin Li** (M2 Data Science_École Polytechnique)\n",
    "- **Thanh-Nam Nguyen** (M2 Data Science_École Polytechnique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Abstract:** Our project was inspired by these articles *\"Estimating NBA players salary share according to their performance on court: A machine learning approach\"*, and the article *\"Does Racial Discrimination Exist Within the NBA? An analysis Based on Salary-per-Contribution\"*, both articles can be found in the repository. The aim of our project is to present a notebook that predicts the salary of NBA players in term of his characteristics.\n",
    "\n",
    "**Source of the Dataset:**\n",
    "- Link: https://figshare.com/articles/dataset/NBA_data/5414170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, at first we focused our attention on the season **2015-2016** to analyse our approach and our methodology and then we added other seasons for comparison purposes and also to perform a new way of cross validating our models.\n",
    "\n",
    "Our raw data consisted of three seperate databases, that again can be found in the repository:\n",
    "\n",
    "- *players cv.xlsx* : this dataset contains personal information about players such as the height, the origin, the race...\n",
    "- *players stat.xlsx* : this dataset contains the statistics concercning the performance of each player.\n",
    "- *players salary.xlsx* : this dataset contains the names of the players, the teams they play in and their respective salaries.\n",
    "\n",
    "We cleaned these three datasets separately and then merged them into one big dataset that we'll preprocess and run through our different models. The dataset contains several basketball statistics of the players and a column of the salary of each player. We'll start by giving a brief explanation of each feature of the dataset. More details about the features will be described later on with the plots. We have divided main features in three different groups which are *\"Player personal characteristics\"*, *\"Player general infromation related to games\"*, and *\"Player performance features\"* along with the target feature which is the `Salary`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Player personal characteristics**\n",
    "> - `Player`: The name of the player\n",
    "> - `Age` Age of the player\n",
    "> - `Tm` Team of the player\n",
    "> - `Place_of_birth` is the place of birth of the player (it can be a state of the USA and that's why we added the following new column `state_or_country`)\n",
    "> - `state_or_country` it indicates whether the place of birth is a state of the USA or a country of the world \n",
    "> - `Race` Race of the player: Black, White, Black and White or other\n",
    "> - `Ht`: Height of the player\n",
    "> - `Wt`: The weight of the player\n",
    "> - `College`: College to which the player went\n",
    "\n",
    "\n",
    "- **Player general information related to games**\n",
    "> - `Pos`: Position in which the player plays on the basketball court\n",
    "> - `Season`: The season of NBA.\n",
    "> - `G`: Games. It the number of games a player has played in.\n",
    "> - `GS`: Game Started. It means the number of games in which a player has started in the game.\n",
    "> - `MP`: Minutes Played\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Player performance features**\n",
    "> - `PER`: Player efficiency rating\n",
    "> - `3APr`: 3-Point Attempt rate\n",
    "> - `FTr`: Free Throw Attempt rate\n",
    "> - `ORB%`: Offensive Rebound Percentage\n",
    "> - `DRB%`: Defensive Rebound Percentage\n",
    "> - `TRB%`: Total Rebound Percentage\n",
    "> - `AST%`: Assist Percentage\n",
    "> - `STL%`: Steal Percentage\n",
    "> - `BLK%`: Block Percentage\n",
    "> - `TOV%`: Turnover Percentage\n",
    "> - `USG%`: Usage Percentage\n",
    "> - `ORtg`: Offensive Rating\n",
    "> - `DRtg`: Defensive Rating\n",
    "> - `OWS`: Offensive Win share\n",
    "> - `DWS`: Defensive Win share\n",
    "> - `WS/48`: Win shares per 48 minutes\n",
    "> - `OBPM`: Offensive Box Plus/Minus\n",
    "> - `DBPM`: Defensive Box Plus/Minus\n",
    "> - `BPM`: Box Plus/Minus\n",
    "> - `VORP`: Value over Replacement Player\n",
    "\n",
    "\n",
    "- **Target**\n",
    "> - `SALARY`: Our target variable that we want to predict: the salary of the player\n",
    "\n",
    "\n",
    "Another idea that popped into our heads for when we will add the other seasons is to take into consideration the factor of inflation in the salaries of the players to make our predictions more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necesarry packages and librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import copy as cp\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from helpers import convert_height\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction of our project, from the following section, we will first dive into the **2015-2016** season.\n",
    "With this dataset, we proceed data visualization, feature engineering and derive conclusions that could be made by analyzing the plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Analysis and Simple Feature Engineering for the season: 2015-2016**\n",
    "\n",
    "- In this section, as an illustration of our approch, we first only study on the dataset of the **2015-2016** season. We merge them into one dataframe, then verify duplicated, NaN and irrelevant values. Some simple feature engineering will also be proposed. More advanced techniques about feature engineering will be presented in next sections when dealing with the wholde dataset of several seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "data_path = \"data/\"\n",
    "player_stat = pd.read_excel(os.path.join(data_path, \"raw_data\", \"players stat.xlsx\"))\n",
    "player_cv = pd.read_excel(os.path.join(data_path, \"raw_data\", \"players cv.xlsx\"))\n",
    "player_salary = pd.read_excel(os.path.join(data_path, \"raw_data\", \"players salary.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricting the dataset to the season 2015-2016\n",
    "player_stat_15_16 = player_stat[player_stat['Season'] == '2015-16']\n",
    "player_salary_15_16 = player_salary[player_salary['SEASON'] == '2015-2016']\n",
    "player_cv_15_16 = player_cv[(player_cv['From']<=2016) & (player_cv['To']>=2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Player stat:\", player_stat_15_16.shape)\n",
    "print(\"Player salary:\", player_salary_15_16.shape)\n",
    "print(\"Player CV:\", player_cv_15_16.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the three datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the datasets `player_CV` and `player_stat` was relatively easy in comparison with merging `player_stat` and `player_salary` because the names in both datasets were written in the same format. We used an inner join because in order to only take the intersection of players and avoid NaN values.\n",
    "\n",
    "Moreover, before merging the two dataframe `player_stat_15_16` and `player_cv_15_16`, we have checked if they contain only the unique players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the datasets player cv and player stat\n",
    "merge_stat_cv = pd.merge(player_stat_15_16, player_cv_15_16, left_on='Player', right_on='Player', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below came as a consequence of a manual filtering of the names in the datasets to cover all the exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps reformat the names in the player_stat database to the same format of player_salary\n",
    "def preprocess_name(name):\n",
    "    if ',' in name:\n",
    "        ind = name.find(',')\n",
    "        name = name[:ind]\n",
    "    ### We remove the Jr.\n",
    "    if \"Jr.\" in name:\n",
    "        name = name.replace(\"Jr.\", \"\")\n",
    "    if \"Jr\" in name:\n",
    "        name = name.replace(\"Jr.\", \"\")\n",
    "    if \"III\" in name:\n",
    "        name = name.replace(\"III\", \"\")\n",
    "    while '.' in name:\n",
    "        ind = name.find('.')\n",
    "        name = name.replace('.','')\n",
    "    name = name.title()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be safe, we apply the same name preprocessing function to the two databases \n",
    "player_salary_15_16['Player'] = player_salary_15_16['NAME'].apply(lambda s: preprocess_name(s))\n",
    "merge_stat_cv['Player'] = merge_stat_cv['Player'].apply(lambda s: preprocess_name(s))\n",
    "\n",
    "# Merging the three datasets\n",
    "merge_15_16 = pd.merge(merge_stat_cv, player_salary_15_16, left_on='Player', right_on='Player', how='inner')\n",
    "merge_15_16.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, all the three datasets were almost of size 500 rows. But after doing the intersection and preprocessing we can observe that the size of the database decreased considerably, because some players existed in a database and not in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_15_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_15_16.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy the merged dataframe in order to proceed with safe trials\n",
    "season_15_16 = merge_15_16.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and trat duplicated, missing values \n",
    "- In this subsection, we will check if there are any NaN values in the dataframe that we will deal with.\n",
    "> - By proceeding the action below, we found out that `3PAr`, `FTr`, `TOV%`, `ORtg`, and `College` has NaN values.\n",
    "> - In order to treat the NaN values, we either dropped or replaced them, which will be shown below.\n",
    "> - We do not perform a simple row drop NaN values since more than 20% rows contains at least one NaN values. Since such operation will cause lossing a lot of information in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of duplicated rows\n",
    "print(f\"Number of duplicated rows are {season_15_16.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the NaN values\n",
    "df_null_15_16 =  season_15_16.isnull().sum(axis=0)\n",
    "df_null_15_16 = df_null_15_16[df_null_15_16>0]\n",
    "print(f\"Below all columns with number of missing values \\n{df_null_15_16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values of \"College\" column to \"Missing\"\n",
    "season_15_16[\"College\"] = season_15_16[\"College\"].fillna(\"Missing\")\n",
    "season_15_16 = season_15_16.dropna(axis=0)\n",
    "season_15_16.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unneccesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are redundant or overlapped\n",
    "to_drop = ['Lg', 'NAME', 'Player', 'SEASON', 'TEAM']\n",
    "season_15_16 =  season_15_16.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We convert height from feet and inch to centimeters to have clearer understanding of the units.\n",
    "2. We created new feature named `seniority` by computing the number of year that the player played in the NBA until the current season.\n",
    "3. Also, we will see below that since the range of the salary is very large, we decided to create a new feature named `SALARY_CATEGORY` which have three different categories of salaries called `Low`, `Medium`, and `High`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_15_16['Ht'] = season_15_16['Ht'].apply(convert_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_seniority = ['To', 'From']\n",
    "season_15_16['seniority'] = season_15_16[columns_for_seniority].apply(lambda row: row[0] - row[1], axis =1)\n",
    "season_15_16 = season_15_16.drop(columns_for_seniority, axis=1)\n",
    "season_15_16.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to divide the salary categories evenly, we have used `qcut`, which will give a balanced division of the three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_15_16[\"SALARY_CATEGORY\"] = pd.qcut(season_15_16[\"SALARY\"], q=3, labels=['Low', 'Medium', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `Pos`(position of the players) to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_15_16[\"Pos\"] = season_15_16[\"Pos\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the positions are all unique\n",
    "season_15_16[\"Pos\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Visualization**\n",
    "- We present the EDA following the order of groups of characteristic of player: Target Feature, Personal characteristics, Player general information related to games, performance features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the target column `SALARY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=season_15_16, x=\"SALARY\", kde=True)\n",
    "plt.title('Distribution of the salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_15_16['SALARY_CATEGORY'].value_counts(normalize=True).plot.bar()\n",
    "plt.xticks(rotation=30)\n",
    "plt.title('Categories of the salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of **Player personal characteristics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of age\n",
    "sns.displot(x=\"Age\", kde=True, data=season_15_16)\n",
    "plt.title(\"Distribution of Age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the boxplot below, we can see that the height of the players are from 180cm to 220cm. Which is higher than the average height of men in USA. And therfore, it is not difficult to understand the information that this feature gives, since tall players will have a lot of advantage to play basketball games. \n",
    "- However, in the boxplot below, we see an oulier in the dataset where one player has really lower height than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the range of the height of players\n",
    "sns.boxplot(y='Ht', data=season_15_16)\n",
    "plt.title(\"Boxplot of Ht\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of **player general information in games** with respect to  `SALARY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of games/minutes the player played and his salary (`G` and `MP` w.r.t. `SALARY`)\n",
    "\n",
    "- Number of games and minutes one player played in the season is an undirect measure for his performance. \n",
    "- In general, the more times he played, the more important his role is. As a consequnce, he would obtain a good salary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- > - The plot below shows the line represention of the mean of salary with respect to the number of game the player already played. \n",
    "> - It shows that the curve relatively increases as a function of `G`.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"G\", y=\"SALARY\", kind=\"line\", data=season_15_16, )\n",
    "plt.title(\"Relationship between number of games per player and their salaries\"+\"\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- > - Using our new feature `SALARY_CATEGORY` and plotting boxplots for `G`, the phenomena is illustrated much more clearly. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"G\", x=\"SALARY_CATEGORY\", data=season_15_16)\n",
    "plt.title(\"Boxplots between number of games per player and their salaries\"+\"\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Using our new feature `SALARY_CATEGORY` and plotting boxplots for `MP`, we could also see that the players who played more minutes earned higher salaries. Which proves the assupmtion that the one who playes more indicates that they are the skilled players, and they will earn high salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"MP\", x=\"SALARY_CATEGORY\", data=season_15_16)\n",
    "plt.title(\"Box plots of number of minutes player played in his salary category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The seniority of the player and salary (`seniority` w.r.t `SALARY`)\n",
    "\n",
    "- Here, we additionally defined a new category of seniority called `seniority_category`, which is divided into two different category which is called `Junior` and `Senior`.\n",
    "- In order to divide the seniority categories evenly, we have used `qcut`, which will give a balanced division of the two categories.\n",
    "- We will see that the senior players have higher salary than the junior ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new category of seniority\n",
    "season_15_16[\"seniority_category\"] = pd.qcut(season_15_16[\"seniority\"], q=2, labels=['Junior', \"Senior\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the barplot below, we can see that the seniors have higher salaries than the juniors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a barplot to see the mean and standard deviation of the salary for two seniority categories\n",
    "sns.barplot(x=\"seniority_category\", y=\"SALARY\", data=season_15_16)\n",
    "plt.title(\"Mean and standard deviation of the salary for junior and senior players\"+\"\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position of the players and salary (`Pos` w.r.t `SALARY`)\n",
    "\n",
    "- From the barplot below, we could see that in average, the players who play the role between the center position and the forward position earns the most among the other positioned players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a barplot to see the mean and standard deviation of the salary of players by their position\n",
    "sns.barplot(x=\"Pos\", y=\"SALARY\", data=season_15_16)\n",
    "plt.title(\"Mean and standard deviation of salary of players by their positions\"+\"\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of **Player performance features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NBA basketball court.PNG\"\n",
    "width = 50%;\n",
    "height = auto; />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PER(Player Efficiency Rate)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What actually is PER?\n",
    "> - PER takes into account <FONT COLOR=\"BLUE\">**accomplishments**</FONT>, such as field goals, free throws, 3-pointers, assists, rebounds, blocks and steals but also includes player's <FONT COLOR=\"RED\">**negative results**</FONT> such as missed shots, turnovers and personal fouls.\n",
    "\n",
    "- From the distribution plot below of the PER(Player Efficiency Rate), \n",
    "> - We can see that some players have negative PER which indicates that this player is extremely inefficient.\n",
    "> - On the contrary, for the players who have a PER of higher than 30 are the ones who have great efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of PER\n",
    "sns.displot(data=season_15_16, x='PER', kde=True)\n",
    "plt.title(\"Histogram of PER\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the `SALARY_CATEGORY`:\n",
    "> - We first visualize the mean PER per salary category as the plot below.\n",
    ">> - From this, we see that the players who have high PER in average have high salaries, players who have low PER in average have low salaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_15_16.groupby('SALARY_CATEGORY').agg({'PER':np.mean}).plot.bar()\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Mean PER per Salary category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3PAr(3-Point Attempt Rate) and FTr(Free Throw Rate)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What actually is 3PAr?\n",
    "> - A player can get a three-point field goal (also 3-pointer) when a field goal in a basketball game was made from beyond the three-point line, a designated arc surrounding the basket.\n",
    "> - During the game when the player shoot the ball, the 3-Point Attempt Rate is a measure of what % of a player's shots come from long-distance, a three-point field goal among the field goal trials.\n",
    "\n",
    "- From the distribution plot below of the 3PAr(3-Point Attempt Rate), \n",
    "> - We can see that majority of the players have low 3PAr, and only a few players have high rates around 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of 3PAr\n",
    "sns.displot(data=season_15_16, x='3PAr')\n",
    "plt.title('Distribution of 3PAr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the boxplot below, we see that the range of the diviation of the `Low` salary category was higher than the other salary categories.\n",
    "- Moreover, we see that there weren't any outliers for all three salary categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='SALARY_CATEGORY', y='3PAr', data=season_15_16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What actually is FTr(Free Throw Rate)?\n",
    "> - A free throw, or foul shot, is an unguarded scoring attempt that a referee awards a basketball player after an opposing team member commits a foul against them, their team, or an official. Free throws provide a basketball team with the opportunity to score points outside of the shot clock during a basketball game.\n",
    "> - Free Throw Rate is the ratio of Free Throw Attempts to Field Goal Attempts.\n",
    "\n",
    "- From the distribution plot below of the FTr(Free Throw Rate), \n",
    "> - We can see that some players have very high FTr around 80% to 100% which shows the players who have low performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of FTr\n",
    "sns.displot(data=season_15_16, x='FTr', kde=True)\n",
    "plt.title('Distribution of FTr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the `SALARY_CATEGORY`:\n",
    "> - We first visualize the mean FTr per salary category as the plot below.\n",
    ">> - From this, we see that the players who have high FTr in average have low salaries, players who have low 3PAr in average have high salaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of FTr w.r.t SALARY_CATEGORY\n",
    "sns.boxplot(x=\"SALARY_CATEGORY\", y = \"FTr\", data=season_15_16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">**Positive performance features**</FONT>\n",
    "- **AST%**: Assist Percentage is an estimate of the percentage of teammate field goals a player assisted\n",
    "- **STL%**: Steal Percentage is an estimate of the percentage of opponent possessions that end with a steal by the player\n",
    "- **BLK%**: Block Percentage is an estimate of the percentage of opponent two-point field goal attempts blocked by the player\n",
    "\n",
    "### <FONT COLOR=\"RED\">**Negative performance features**</FONT>\n",
    "- **TOV%**: Turnover Percentage is an estimate of turnovers committed per 100 plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below is provided to help the understanding of the positions of players in a basketball match.\n",
    "\n",
    "The basketball player positions are mainly composed as below:\n",
    "> - Center\n",
    "> - Point Guard\n",
    "> - Shooting Guard\n",
    "> - Small Forward\n",
    "> - Power Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Position.jpg\"\n",
    "width = 45%;\n",
    "height = auto; />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(15,15))\n",
    "fig.suptitle('Box plot of positive and negative performance features')\n",
    "sns.boxplot(x='Pos', y='AST%', data=season_15_16, ax=axs[0,0])\n",
    "sns.boxplot(x='Pos', y='STL%', data=season_15_16, ax=axs[0,1])\n",
    "sns.boxplot(x='Pos', y='BLK%', data=season_15_16, ax=axs[1,0])\n",
    "sns.boxplot(x='Pos', y='TOV%', data=season_15_16, ax=axs[1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Relationship between various features: `TRB%`, `STL%`, `SALARY`,...**\n",
    "- In order to see the general relationships between various features, we used the `pairplot` for better visualization.\n",
    "\n",
    "*Some aspects we can see from the pairplot below:*\n",
    "> Linear distribution between certain features: \n",
    "> - 1. `AST%` and `STL%`: Considering the fact that both `AST%` and `STL%` are positive player performance features, we can see that the player who has higher assist rate also has higher steal rate. This could be assumed by the definition of \"assist\" in playing basketball in situations when the player steal the ball from the opponent and pass it to their team's player and the player who received the ball shoots a goal. Then the player who passed the ball has \"assisted\" the goal, and thus will have high `AST%`.\n",
    "> - 2. `PER` and `ORtg`: Due to the fact that the offensive rating(`ORtg`) is an estimate of points produced by players or scored by teams per 100 possessions, and the PER is a more global parameter that indicates the player's efficiency rate, we could see that the players who produce more points will highly have higher PER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    season_15_16,\n",
    "    x_vars=[\"TRB%\", \"AST%\", \"STL%\", \"BLK%\", \"PER\", \"3PAr\", \"FTr\", \"ORtg\", \"DRtg\"],\n",
    "    y_vars=[\"TRB%\", \"AST%\", \"STL%\", \"BLK%\", \"PER\", \"3PAr\", \"FTr\", \"ORtg\", \"DRtg\"],\n",
    "    #hue = \"SALARY_CATEGORY\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparison of performance of between players** using the \"Radar plot\"\n",
    "\n",
    "- Since many features are used to take into account all aspects of the player performance. \n",
    "- We would like to visualize and compare main features at same time between the player with highest and the one with lowest salary. \n",
    "\n",
    "It turns out that the area generated by performance features for higher salary players would be larger than that of lower salary players. This nature is clear since higher the salary, one player has, the better he is, and hence the better skills, he posseses.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = [\"AST%\", \"STL%\", \"BLK%\", \"USG%\", \"TRB%\"]\n",
    "season_15_16_sort = season_15_16.sort_values(by=\"SALARY_CATEGORY\", ascending=False) \n",
    "player_hightest_salary = dict(season_15_16_sort.iloc[0][columns_to_plot])\n",
    "player_lowest_salary = dict(season_15_16_sort.iloc[-1][columns_to_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_and_angles(player_info: dict):\n",
    "    Attributes = list(player_info.keys())\n",
    "    AttNo = len(Attributes)\n",
    "    \n",
    "    values = list(player_info.values())\n",
    "    values += values [:1]\n",
    "    \n",
    "    angles = [n / float(AttNo) * 2 * np.pi for n in range(AttNo)]\n",
    "    angles += angles [:1]\n",
    "    return angles, values, Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rada(player_info: dict, label):\n",
    "    \n",
    "    angles, values, Attributes = get_values_and_angles(player_info)\n",
    "\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    #Add the attribute labels to our axes\n",
    "    plt.xticks(angles[:-1], Attributes)\n",
    "\n",
    "    #Plot the line around the outside of the filled area, using the angles and values calculated before\n",
    "    ax.plot(angles,values)\n",
    "\n",
    "    #Fill in the area plotted in the last line\n",
    "    ax.fill(angles, values, 'teal', alpha=0.1)\n",
    "\n",
    "    #Give the plot a title and show it\n",
    "    ax.set_title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rada(player_lowest_salary, \"Lowest salary player\")\n",
    "plot_rada(player_hightest_salary, \"Highest salary player\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rada_2players(player1_info: dict, player2_info: dict, labels):\n",
    "    \n",
    "    angles1, values1, Attributes = get_values_and_angles(player1_info)\n",
    "    angles2, values2, _ = get_values_and_angles(player2_info)\n",
    "    \n",
    "\n",
    "    #Create the chart as before, but with both Ronaldo's and Messi's angles/values\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_title(\"Comparison of performances between the lowest and highest salary players\")\n",
    "\n",
    "    plt.xticks(angles1[:-1], Attributes)\n",
    "\n",
    "    ax.plot(angles1,values1)\n",
    "    ax.fill(angles1, values1, 'blue', alpha=0.1)\n",
    "\n",
    "    ax.plot(angles2,values2)\n",
    "    ax.fill(angles2, values2, 'red', alpha=0.1)\n",
    "\n",
    "    #Rather than use a title, individual text points are added\n",
    "    plt.figtext(0.2,0.85, labels[0], color=\"red\")\n",
    "    plt.figtext(0.2,0.82,\"vs\")\n",
    "    plt.figtext(0.2,0.80, labels[1], color=\"blue\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_rada_2players(player_lowest_salary, player_hightest_salary, [\"Lowest salary player\", \"Hightest salary player\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Relationship between college and salary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have plotted the relationship between colleges and the salary of the players. \n",
    "\n",
    "The plots below will show that the players who went to college with a good basketball team such as \"University of Kentucky\" or \"Arizona State University\" did not always have players who have high salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the original dataset and create a new dataframe of salaries to remove \"Missing\" values\n",
    "college_clean = cp.deepcopy(season_15_16)\n",
    "\n",
    "index_names = college_clean[ college_clean['College'] == 'Missing'].index\n",
    "college_clean.drop(index_names, inplace = True)\n",
    "\n",
    "college_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to see the relationship between College and the salary\n",
    "plt.figure(figsize=(18,10))\n",
    "sns.scatterplot('College', 'SALARY', data=college_clean, hue='College')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Relationship between College and the salary of the players')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finalize if the assumption is false, we also tried to see the list of colleges of the top 20 players who have high salaries during this season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the original dataset and create a new dataframe of salaries with descending order\n",
    "college_asc = cp.deepcopy(college_clean)\n",
    "college_asc = college_asc.sort_values(by=['SALARY'], ascending=False)\n",
    "college_asc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='SALARY',\n",
    "    y='College',\n",
    "    data=college_asc.nlargest(20, 'SALARY'),\n",
    "    hue='Pos'\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Colleges of top 20 players with highest salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the top 20 players who have the highest salary did not come from the colleges that has a good basketball team, and therefore the assuption we made above is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Adding other seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_14_15 = pd.read_csv(os.path.join(data_path, \"preprocessed_data\", \"merge_14_15.csv\"))\n",
    "merge_13_14 = pd.read_csv(os.path.join(data_path, \"preprocessed_data\", \"merge_13_14.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be more accurate, we integrated the inflation factor in the salary column for the different years. We take as reference the year 2016. According to this website: https://stats.areppim.com/calc/calc_usdlrxdeflator.php, 1.00 US Dollars of 2014 are worth 1.02 US Dollars of 2016 and 1.00 US Dollars of 2015 are worth 1.01 US Dollars of 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_15_16['SALARY'] = merge_15_16['SALARY'] * 1.0\n",
    "merge_14_15['SALARY'] = merge_14_15['SALARY'] * 1.01\n",
    "merge_13_14['SALARY'] = merge_13_14['SALARY'] * 1.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([merge_15_16, merge_14_15, merge_13_14]).reset_index(drop=True)\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns **NAME** and **SEASON** and **TEAM** are repeated twice so we can remove the duplicated columns. Also, we can remove the column of the league because they all play for the NBA for these seasons. Finally, we drop the column of the player's names because it's a categorical column that takes a different value for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We made a deepcopy of the dataset to keep the original untouched since we will modify this one in preprocessing\n",
    "M = cp.deepcopy(final_df)\n",
    "to_drop = ['Lg', 'NAME', 'Player', 'SEASON', 'TEAM']\n",
    "M.drop(to_drop, axis=1, inplace=True)\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our dataset contains a lot of important categorical features like the basketball team of the players and their colleges... So it's important to find the most efficient way to encode them. During the week of the data camp, we saw many encoding techniques and that this step is critical in our preprocessing since it can influence significantly our model performance.\n",
    "\n",
    "We decided to use the Count Ordinal Encoder for all categorical variables because:\n",
    "\n",
    "- We could've used the Ordinal encoding, but we didn't because it can introduce bias and give for example a race superiority over another for the race column. \n",
    "\n",
    "\n",
    "- We didn't use Count Encoding because it can be a source of collisions. \n",
    "\n",
    "\n",
    "- We didn't use One hot encoding because during the week of the data camp we saw that it's mostly used with linear algorithms. Moreover, it can lead to sparse representations if we have many categories like in **Place_of_Birth** and **College**.\n",
    "\n",
    "\n",
    "- Finally,  we chose the Count Ordinal Encoding because it works for both linear and non linear algorithms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import CountOrdinalEncoder\n",
    "## We use the code from the Datacamp week for the CountOrdinalEncoder\n",
    "coe = CountOrdinalEncoder()\n",
    "new_c = coe.fit_transform(pd.DataFrame(M['Tm']))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('Tm', axis=1, inplace=True)\n",
    "M.insert(1, 'Tm', new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial height values are feet mixed with inches and we want a more homogenous value, so we convert it to centimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting height from feet to centimeters\n",
    "from helpers import convert_height\n",
    "new_c = np.array(M['Ht'].apply(convert_height))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('Ht', axis=1, inplace=True)\n",
    "M.insert(32, 'Ht', new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataframe contained the first year in which the player started playing (*From*) up to the year of the season (*To*). In order to better use this information, we came up with the idea of taking the difference. We can interpret this new column as the seniority of the player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"From\" and \"To\" columns are redundant, we can just convert them to a column seniority\n",
    "duration = M['To'] - M['From']\n",
    "M.insert(29, 'seniority', duration)\n",
    "M.drop('To', axis=1, inplace=True)\n",
    "M.drop('From', axis=1, inplace=True)\n",
    "\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the race into a numerical value\n",
    "pd.Categorical(M['Race']).categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_c = coe.fit_transform(pd.DataFrame(M['Race']))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('Race', axis=1, inplace=True)\n",
    "M.insert(28, 'Race', new_c)\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will work on the Place of Birth of the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(M['Place_of_Birth'].unique()))\n",
    "print(len(M['Place_of_Birth'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some states and countries appeared twice because of lower/uppercase differences and extra spaces at the end.\n",
    "For example, we have `'Missouri'` and  `'Missouri '`, `'Serbia'` and `'serbia'` etc. So, we decided to add some preprocessing to put it all in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_names(s):\n",
    "    s = s.lower().strip() #lowercase and remove the extra space at the end\n",
    "    return s\n",
    "\n",
    "M['Place_of_Birth'] = M['Place_of_Birth'].apply(lambda s: correct_names(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a misspell in the name of **Wisconsin** that need to be corrected. Also, **Russia** and **Russian Federation** mean the same country so we need to correct it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Federation of Russia to Russia\n",
    "indices = np.where(M['Place_of_Birth'] == \"russian federation\")[0]\n",
    "M['Place_of_Birth'][indices] = \"russia\"\n",
    "\n",
    "# Correcting Wisconsin\n",
    "indices = np.where(M['Place_of_Birth'] == \"wisvonsin\")[0]\n",
    "M['Place_of_Birth'][indices] = \"wisconsin\"\n",
    "\n",
    "print(np.sort(M['Place_of_Birth'].unique()))\n",
    "print(len(M['Place_of_Birth'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that we have a mix between states of the USA and countries of the world as places of birth of the players. So, we will add another column to specify if it's a state of the USA or an actual country. This column will be categorical and it will take 1 if the place of birth is a state of the USA and 0 if it's a country of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida','Georgia',\n",
    "          'Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts',\n",
    "          'Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey',\n",
    "          'New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "          'South Carolina','South Dakota','Tennessee','Texas','Utah','Vermont','Virginia','Washington','West Virginia',\n",
    "          'Wisconsin','Wyoming']\n",
    "states = [state.lower() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.insert(28,'state_or_country',M['Place_of_Birth'].isin(states) * 1)\n",
    "\n",
    "# We encode the place of birth by count ordinal encoding\n",
    "new_c = coe.fit_transform(pd.DataFrame(M['Place_of_Birth']))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('Place_of_Birth', axis=1, inplace=True)\n",
    "M.insert(27, 'Place_of_Birth', new_c)\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All possible positions are:', list(pd.Categorical(M['Pos']).categories))\n",
    "\n",
    "# Encoding positions\n",
    "new_c = coe.fit_transform(pd.DataFrame(M['Pos']))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('Pos', axis=1, inplace=True)\n",
    "M.insert(31, 'Pos', new_c)\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All possible colleges are:', list(pd.Categorical(M['College']).categories))\n",
    "print('\\n\\nNumber of colleges: ', len(list(pd.Categorical(M['College']).categories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will encode the college column using the COE as before. In fact, we believe this feature can be important since colleges with good basketball teams tend to offer scholarships to high school students that have good performance in this sport. Consequently, a player with a high salary is more likely to have studied in a college with a good basketball team. This assumption is to be confirmed by plots shown later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating NAN values of this column\n",
    "# We will just replace all this players of whom we don't have info about the college with \"Missing\"      \n",
    "M['College'][np.where(M['College'].isna() == True)[0]] = \"Missing\"\n",
    "new_c = coe.fit_transform(pd.DataFrame(M['College']))\n",
    "new_c = new_c.reshape(len(new_c),)\n",
    "M.drop('College', axis=1, inplace=True)\n",
    "M.insert(34, 'College', new_c)\n",
    "\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop **Date of Birth**, since it's redundant because we already have the age of each player stored in a column.\n",
    "We also drop the column **RK**. In fact, this column represents the ranking of the players based on their salary, so we cannot use it as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.drop('Birth Date', axis=1, inplace=True)\n",
    "M.drop('RK', axis=1, inplace=True)\n",
    "\n",
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we have NaN values in our dataframe. Let's investigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M[M.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the missing values are located only in two rows. Morever, since the missing values are key performance features for the two players, replacing these values with the mean (or median) might induce predictions errors. Instead, we decided to simply drop these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.where(M['3PAr'].isna())[0]\n",
    "M.drop(ind, axis=0, inplace=True)\n",
    "M.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing step is now over, the next step will be the data analysis which will help us select the features for salary prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M[M.columns].corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "sns.heatmap(M[M.columns].corr(), annot=True, cmap = 'Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high correlations suggest that many of the columns contain redundant information, i.e. information from one column is contained in other columns. We can only use a subset of the columns for training and predicting. However, we chose not drop any column since a regularized model can do feature selection on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we wanted to see what features are the most discriminative via a Lasso regression. To do so, we decided at first to limit ourselves to one season. Afterwards, we will code a more sophisticated way  of splitting which will be more adapted to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into tagert and learning data\n",
    "M_15_16 = M[M['Season']=='2015-16']\n",
    "Y = cp.deepcopy(M_15_16['SALARY'])\n",
    "col = list(set((M_15_16.columns))- set(['SALARY', 'Season']))\n",
    "X = cp.deepcopy(M_15_16[col])\n",
    "\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "# Splitting into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lasso = 100\n",
    "weights_lasso, MSE_lasso, MSE_lasso_app = list(), list(), list()\n",
    "alphas_lasso = np.logspace(-3, 5, n_lasso)\n",
    "for a in alphas_lasso:\n",
    "    lasso = Lasso(alpha = a)\n",
    "    lasso.fit(X_train_scaled, y_train_scaled)\n",
    "    weights_lasso.append(lasso.coef_)\n",
    "    y = lasso.predict(X_test_scaled)\n",
    "    y_app = lasso.predict(X_train_scaled)\n",
    "    MSE_lasso.append(mean_squared_error(y, y_test_scaled))\n",
    "    MSE_lasso_app.append(mean_squared_error(y_app, y_train_scaled))\n",
    "weights_lasso = np.array(weights_lasso)\n",
    "\n",
    "pl.figure(11, figsize=(20, 6))\n",
    "ax = pl.gca()\n",
    "for i in range(weights_lasso.shape[1]):\n",
    "    ax.plot(alphas_lasso, weights_lasso[:,i])\n",
    "ax.set_xscale('log')\n",
    "pl.xlabel('alpha')\n",
    "pl.ylabel('weights')\n",
    "pl.title('Lasso coefficients as a function of the regularization', fontsize = 20)\n",
    "pl.grid()\n",
    "pl.axis('tight')\n",
    "\n",
    "pl.figure(12, figsize=(20, 6))\n",
    "pl.subplot(1,2,1)\n",
    "ax = pl.gca()\n",
    "ax.plot(alphas_lasso, MSE_lasso)\n",
    "ax.scatter(alphas_lasso[np.argmin(MSE_lasso)], np.amin(MSE_lasso), c = 'red')\n",
    "ax.set_xscale('log')\n",
    "pl.xlabel('alpha')\n",
    "pl.ylabel('MSE')\n",
    "pl.title('MSE as a function of the regularization (testing data)', fontsize = 20)\n",
    "pl.grid()\n",
    "pl.axis('tight')\n",
    "pl.subplot(1,2,2)\n",
    "ax = pl.gca()\n",
    "ax.plot(alphas_lasso, MSE_lasso_app)\n",
    "ax.scatter(alphas_lasso[np.argmin(MSE_lasso_app)], np.amin(MSE_lasso_app), c = 'red')\n",
    "ax.set_xscale('log')\n",
    "pl.xlabel('alpha')\n",
    "pl.ylabel('MSE')\n",
    "pl.title('MSE as a function of the regularization (training data)', fontsize = 20)\n",
    "pl.grid()\n",
    "pl.axis('tight')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The top plot represents the regularization path. This illustrates the fact that Lasso performs feature selection.\n",
    "\n",
    "\n",
    "- The two bottom plots show the optimal $\\lambda$ values for on test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_optim_lasso = alphas_lasso[np.argmin(MSE_lasso)]\n",
    "model_lasso_optim = Lasso(alpha = alpha_optim_lasso)\n",
    "model_lasso_optim.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\nThe value of alpha that gives the smallest MSE score is: {}\".format(np.round(alpha_optim_lasso,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the selection made by Lasso and use it to do variable selection and apply it to a non linear model\n",
    "pl.figure(14, figsize = (20,8))\n",
    "x = np.array([i+1 for i in range(model_lasso_optim.coef_.shape[0])])\n",
    "pl.subplot(1,2,1)\n",
    "pl.tight_layout(h_pad = 5, w_pad = 5)\n",
    "pl.stem(x, model_lasso_optim.coef_, use_line_collection = True)\n",
    "pl.title(\"Stem plot of the values of the weights of the Lasso model\",fontsize=20)\n",
    "pl.xlabel(\"The weights\",fontsize=15)\n",
    "pl.ylabel(\"Values of the weights\",fontsize=15)\n",
    "pl.grid()\n",
    "pl.subplot(1,2,2)\n",
    "pl.stem(x, np.abs(model_lasso_optim.coef_), use_line_collection = True)\n",
    "pl.title(\"Stem plot of the absolute value of the weights of the Lasso model\",fontsize=20)\n",
    "pl.xlabel(\"The weights\",fontsize=15)\n",
    "pl.ylabel(\"Absolute values of the weights\",fontsize=15)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The corresponding features of the stem plot:\\n', col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the performance of our first model on the test set. To do so, we chose the MSE, which is the standard metric for regression. Moreover, to relatively quantify the error made by the model, we used a scale-free metric such as the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model_lasso_optim.predict(X_test_scaled)\n",
    "print(\"The MSE score is:\", mean_squared_error(y_test_scaled, y_predicted))\n",
    "print(\"The R2 score is:\", r2_score(y_test_scaled, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wanted to use a non linear model because they are generally more performant than linear models like Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rd = XGBRegressor()\n",
    "model_rd.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = model_rd.predict(X_test_scaled)\n",
    "print(\"The MSE score is:\", mean_squared_error(y_test_scaled, y_pred))\n",
    "print(\"The R2 score is:\", r2_score(y_test_scaled, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the Lasso regression is more accurate than the XGBRegressor with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to define our own cross validation class. Instead of splitting our data randomly as we often do in regular dataframes, we took into consideration the season information. Each time we keep aside one season for the test and train the model on the remaining seasons. \n",
    "\n",
    "The aim of this type of cross validation is to test the robustness of our model for when we change seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "class SeasonSplit(BaseCrossValidator):\n",
    "\n",
    "    def __init__(self, season_col='Season'):  \n",
    "        self.season_col = season_col\n",
    "\n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        return len(X.reset_index()[self.season_col].unique())\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        n_splits = self.get_n_splits(X, y, groups)\n",
    "        X = X.reset_index()\n",
    "        season_names = X[self.season_col].unique()\n",
    "        for i in range(n_splits):\n",
    "            test_season = season_names[i]\n",
    "            \n",
    "            idx_test = list(X[X[self.season_col]==test_season].index)\n",
    "            \n",
    "            idx_train = list(set(X.index) - set(idx_test))\n",
    "            yield (\n",
    "                idx_train, idx_test\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test our splitter that performs cross validation on seasons from 2013-14 to 2015-16 and as a regressor we use XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set((M.columns))- set(['SALARY']))\n",
    "X, y = M[features], M['SALARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.set_index(['index', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rd = XGBRegressor(random_state=0)\n",
    "season_cv = SeasonSplit()\n",
    "print(cross_val_score(model_rd, X, y, cv=season_cv, scoring='r2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = next(season_cv.split(X,y))\n",
    "train = M.iloc[train_idx]\n",
    "test = M.iloc[test_idx]\n",
    "\n",
    "train.to_csv(os.path.join(data_path, \"train\", \"train.csv\"), index=False)\n",
    "test.to_csv(os.path.join(data_path, \"test\", \"test.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
